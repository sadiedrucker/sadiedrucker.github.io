tuesdata <- tidytuesdayR::tt_load('2025-02-11')
library(tidyverse)
library(rvest)
library(janitor)
library(httr2)
library(stringr)
index <- rvest::read_html("https://archive.org/download/20250128-cdc-datasets")
meta_urls <- index |>
rvest::html_element(".download-directory-listing") |>
rvest::html_table() |>
janitor::clean_names() |>
dplyr::filter(stringr::str_ends(x1, "-meta.csv")) |>
dplyr::mutate(
url = paste0(
"https://archive.org/download/20250128-cdc-datasets/",
URLencode(x1)
)
) |>
dplyr::select(url)
rm(index)
# As of 2025-02-03, there are 1257 metadata CSVs available. We will load each
# one and widen it, then stitch them all together. This can take a very long
# time.
requests <- meta_urls$url |>
purrr::map(\(url) {
httr2::request(url) |>
httr2::req_retry(
max_tries = 10,
is_transient = \(resp) {
httr2::resp_status(resp) %in% c(429, 500, 503)
},
# Always wait 10 seconds to retry. It seems to be a general throttle,
# but they don't tell us how long they need us to back off.
backoff = \(i) 10
)
})
resps <- httr2::req_perform_sequential(requests, on_error = "continue")
reqs_to_retry <- resps |>
httr2::resps_failures() |>
purrr::map("request")
resps2 <- httr2::req_perform_sequential(reqs_to_retry)
resps <- c(httr2::resps_successes(resps), httr2::resps_successes(resps2))
extract_cdc_dataset_row <- function(resp) {
httr2::resp_body_string(resp) |>
stringr::str_trim()
}
cdc_datasets <- tibble::tibble(
dataset_url = purrr::map_chr(resps, c("request", "url")),
raw = httr2::resps_data(resps, extract_cdc_dataset_row)
) |>
tidyr::separate_longer_delim(raw, delim = "\r\n") |>
dplyr::filter(stringr::str_detect(raw, ",")) |>
tidyr::separate_wider_delim(
raw,
delim = ",",
names = c("field", "value"),
too_many = "merge",
too_few = "align_start"
) |>
# Remove opening/closing quotes and trailing commas.
dplyr::mutate(
value = stringr::str_trim(value),
value = dplyr::if_else(
stringr::str_starts(value, '"') & stringr::str_ends(value, '"') &
!stringr::str_detect(stringr::str_sub(value, 2, -2), '"'),
stringr::str_sub(value, 2, -2),
value
) |>
stringr::str_remove(",\\s*$") |>
dplyr::na_if("") |>
dplyr::na_if("NA") |>
dplyr::na_if("n/a") |>
dplyr::na_if("N/A")
) |>
dplyr::distinct() |>
dplyr::filter(!is.na(value)) |>
tidyr::pivot_wider(
id_cols = c(dataset_url),
names_from = field,
values_from = value,
# Paste the contents of multi-value fields together.
values_fn = \(x) {
paste(unique(x), collapse = "\n")
}
) |>
janitor::clean_names() |>
dplyr::mutate(
tags = purrr::map2_chr(tags, theme, \(tags, theme) {
if (!is.na(theme)) {
paste(tags, theme, sep = ", ")
} else {
tags
}
}),
language = dplyr::case_match(
language,
"English" ~ "en-US",
.default = language
)
) |>
dplyr::mutate(
dplyr::across(
c("public_access_level", "update_frequency"),
tolower
)
) |>
# Manually dropped identified meaningless columns.
dplyr::select(
-resource_name,
-system_of_records,
-theme,
-is_quality_data
)
omb_codes <- readr::read_csv("https://resources.data.gov/schemas/dcat-us/v1.1/omb_bureau_codes.csv") |>
janitor::clean_names() |>
dplyr::mutate(
cgac_code = dplyr::na_if(cgac_code, "n/a")
)
fpi_codes <- readr::read_csv("https://resources.data.gov/schemas/dcat-us/v1.1/FederalProgramInventory_FY13_MachineReadable_091613.csv") |>
janitor::clean_names()
library(dplyr)
library(ggplot2)
datasets_by_category <- cdc_datasets |>
group_by(category)|>
summarize(n_datasets = n()) |>
arrange(desc(n_datasets))
top_categories <- head(datasets_by_category, 10)
ggplot(top_categories, aes(x = reorder(category, n_datasets), y = n_datasets)) +
geom_col(fill="pink") +
coord_flip() +
labs(
title = "Top 10 dataset categories",
x = "Category",
y = "Number of Datasets"
) +
theme_minimal()
tuesdata <- tidytuesdayR::tt_load('2025-02-11')
library(tidyverse)
library(rvest)
library(janitor)
library(httr2)
library(stringr)
index <- rvest::read_html("https://archive.org/download/20250128-cdc-datasets")
meta_urls <- index |>
rvest::html_element(".download-directory-listing") |>
rvest::html_table() |>
janitor::clean_names() |>
dplyr::filter(stringr::str_ends(x1, "-meta.csv")) |>
dplyr::mutate(
url = paste0(
"https://archive.org/download/20250128-cdc-datasets/",
URLencode(x1)
)
) |>
dplyr::select(url)
rm(index)
# As of 2025-02-03, there are 1257 metadata CSVs available. We will load each
# one and widen it, then stitch them all together. This can take a very long
# time.
requests <- meta_urls$url |>
purrr::map(\(url) {
httr2::request(url) |>
httr2::req_retry(
max_tries = 10,
is_transient = \(resp) {
httr2::resp_status(resp) %in% c(429, 500, 503)
},
# Always wait 10 seconds to retry. It seems to be a general throttle,
# but they don't tell us how long they need us to back off.
backoff = \(i) 10
)
})
resps <- httr2::req_perform_sequential(requests, on_error = "continue")
reqs_to_retry <- resps |>
httr2::resps_failures() |>
purrr::map("request")
resps2 <- httr2::req_perform_sequential(reqs_to_retry)
resps <- c(httr2::resps_successes(resps), httr2::resps_successes(resps2))
extract_cdc_dataset_row <- function(resp) {
httr2::resp_body_string(resp) |>
stringr::str_trim()
}
cdc_datasets <- tibble::tibble(
dataset_url = purrr::map_chr(resps, c("request", "url")),
raw = httr2::resps_data(resps, extract_cdc_dataset_row)
) |>
tidyr::separate_longer_delim(raw, delim = "\r\n") |>
dplyr::filter(stringr::str_detect(raw, ",")) |>
tidyr::separate_wider_delim(
raw,
delim = ",",
names = c("field", "value"),
too_many = "merge",
too_few = "align_start"
) |>
# Remove opening/closing quotes and trailing commas.
dplyr::mutate(
value = stringr::str_trim(value),
value = dplyr::if_else(
stringr::str_starts(value, '"') & stringr::str_ends(value, '"') &
!stringr::str_detect(stringr::str_sub(value, 2, -2), '"'),
stringr::str_sub(value, 2, -2),
value
) |>
stringr::str_remove(",\\s*$") |>
dplyr::na_if("") |>
dplyr::na_if("NA") |>
dplyr::na_if("n/a") |>
dplyr::na_if("N/A")
) |>
dplyr::distinct() |>
dplyr::filter(!is.na(value)) |>
tidyr::pivot_wider(
id_cols = c(dataset_url),
names_from = field,
values_from = value,
# Paste the contents of multi-value fields together.
values_fn = \(x) {
paste(unique(x), collapse = "\n")
}
) |>
janitor::clean_names() |>
dplyr::mutate(
tags = purrr::map2_chr(tags, theme, \(tags, theme) {
if (!is.na(theme)) {
paste(tags, theme, sep = ", ")
} else {
tags
}
}),
language = dplyr::case_match(
language,
"English" ~ "en-US",
.default = language
)
) |>
dplyr::mutate(
dplyr::across(
c("public_access_level", "update_frequency"),
tolower
)
) |>
# Manually dropped identified meaningless columns.
dplyr::select(
-resource_name,
-system_of_records,
-theme,
-is_quality_data
)
omb_codes <- readr::read_csv("https://resources.data.gov/schemas/dcat-us/v1.1/omb_bureau_codes.csv") |>
janitor::clean_names() |>
dplyr::mutate(
cgac_code = dplyr::na_if(cgac_code, "n/a")
)
fpi_codes <- readr::read_csv("https://resources.data.gov/schemas/dcat-us/v1.1/FederalProgramInventory_FY13_MachineReadable_091613.csv") |>
janitor::clean_names()
file.edit("~/.Renviron")
con_traffic <- DBI::dbConnect(
RMariaDB::MariaDB(),
dbname = "traffic",
host = Sys.getenv("TRAFFIC_HOST"),
user = Sys.getenv("TRAFFIC_USER"),
password = Sys.getenv("TRAFFIC_PWD")
)
con_traffic <- DBI::dbConnect(
RMariaDB::MariaDB(),
dbname = "traffic",
host = Sys.getenv("TRAFFIC_HOST"),
user = Sys.getenv("TRAFFIC_USER"),
password = Sys.getenv("TRAFFIC_PWD")
)
install.packages("RMariaDB")
con_traffic <- DBI::dbConnect(
RMariaDB::MariaDB(),
dbname = "traffic",
host = Sys.getenv("TRAFFIC_HOST"),
user = Sys.getenv("TRAFFIC_USER"),
password = Sys.getenv("TRAFFIC_PWD")
)
con_traffic <- DBI::dbConnect(
RMariaDB::MariaDB(),
dbname = "traffic",
host = Sys.getenv("TRAFFIC_HOST"),
user = Sys.getenv("TRAFFIC_USER"),
password = Sys.getenv("TRAFFIC_PWD")
)
con_traffic <- DBI::dbConnect(
RMariaDB::MariaDB(),
dbname = "traffic",
host = Sys.getenv("TRAFFIC_HOST"),
user = Sys.getenv("TRAFFIC_USER"),
password = Sys.getenv("TRAFFIC_PWD")
)
View age_of_stops
View(age_of_stops)
library(tidyverse)
age_of_stops |>
ggplot(aes(x=stop_hour, y=avg_age)) +
geom_point()+
labs(
x="Hour Traffic Stop Occured"
y="Average Age of Subject"
library(tidyverse)
age_of_stops |>
ggplot(aes(x=stop_hour, y=avg_age)) +
geom_point()+
labs(
x="Hour Traffic Stop Occured",
y="Average Age of Subject"
)
library(tidyverse)
age_of_stops |>
ggplot(aes(x=stop_hour, y=avg_age)) +
geom_point()+
labs(
x="Hour Traffic Stop Occured",
y="Average Age of Subject"
)+
theme_minimal()
race_and_arrests |>
ggplot(aes(x = race, y = pct_arrest)) +
geom_col() +
labs(
title = "Arrest Rate by Race in Raleigh Traffic Stops",
x = "Race",
y = "Percent of Stops Ending in Arrest"
) +
theme_minimal() +
coord_flip()
race_and_arrests |>
ggplot() +
geom_col(aes(x = race, y = total_stops), fill = "gray70") +
geom_col(aes(x = race, y = n_arrests), fill = "steelblue") +
labs(
title = "Total Stops and Arrests by Race",
x = "Race",
y = "Count"
) +
theme_minimal()
race_and_arrests |>
ggplot(aes(x = race, y = pct_arrest)) +
geom_segment(aes(x = race, xend = race, y = 0, yend = pct_arrest), color = "gray") +
geom_point(size = 4, color = "steelblue") +
labs(
title = "Arrest Rate by Race",
x = "Race",
y = "Percent Arrested"
) +
theme_minimal()
race_and_arrests |>
ggplot( aes(x = race, y = pct_arrest)) +
geom_col(fill = "steelblue") +
labs(
title = "Arrest Rate by Race in Raleigh Traffic Stops",
x = "Race",
y = "Percent of Stops Ending in Arrest"
) +
theme_minimal() +
coord_flip()
library(forcats)
race_and_arrests |>
ggplot( aes(x = fct_reorder(race, pct_arrest), y = pct_arrest)) +
geom_col(fill = "steelblue") +
labs(
title = "Arrest Rate by Race in Raleigh Traffic Stops",
x = "Race",
y = "Percent of Stops Ending in Arrest"
) +
theme_minimal() +
coord_flip()
library(tidyverse)
age_of_stops |>
ggplot(aes(x = stop_hour, y = avg_age)) +
geom_line(color = "steelblue", size = 1) +
geom_point(color = "darkblue") +
labs(
x = "Hour Traffic Stop Occurred",
y = "Average Age of Subject",
title = "Average Age of Drivers by Hour of Traffic Stops"
) +
theme_minimal()
library(tidyverse)
age_of_stops |>
ggplot(aes(x = stop_hour, y = avg_age)) +
geom_line(color = "lightblue", size = 1) +
geom_point(color = "darkblue") +
labs(
x = "Hour Traffic Stop Occurred",
y = "Average Age of Subject",
title = "Average Age of Drivers by Hour of Traffic Stops"
) +
theme_minimal()
sex_and_citations |>
ggplot(aes(x = sex, y = n_citations, fill = sex)) +
geom_col() +
labs(
title = "Citations Issued by Sex",
x = "Sex",
y = "Number of Citations"
) +
theme_minimal() +
scale_fill_manual(values = c("female" = "pink", "male" = "steelblue"))
sex_and_citations |>
ggplot(aes(x = sex, y = n_citations, fill = sex)) +
geom_col() +
labs(
title = "Citations Issued by Sex",
x = "Sex",
y = "Number of Citations"
) +
theme_minimal() +
scale_fill_manual(values = c("female" = "pink", "male" = "steelblue"))
sex_and_citations |>
ggplot(aes(x = sex, y = pct_of_total_citations, fill = sex)) +
geom_col() +
labs(
title = "Percentage of Citations by Sex",
x = "Sex",
y = "Percent of Total Citations"
) +
theme_minimal() +
scale_fill_manual(values = c("female" = "pink", "male" = "steelblue"))
library(tidyverse)
View(age_of_stops)
age_of_stops |>
ggplot(aes(x = stop_hour, y = avg_age)) +
geom_line(color = "lightblue", size = 1) +
geom_point(color = "darkblue") +
labs(
x = "Hour Traffic Stop Occurred",
y = "Average Age of Subject",
title = "Average Age of Drivers by Hour of Traffic Stops"
) +
theme_minimal()
library(tidyverse)
View(age_of_stops)
age_of_stops |>
ggplot(aes(x = stop_hour, y = avg_age)) +
geom_line(color = "lightblue") +
geom_point(color = "darkblue") +
labs(
x = "Hour Traffic Stop Occurred",
y = "Average Age of Subject",
title = "Average Age of Drivers by Hour of Traffic Stops"
) +
theme_minimal()
age_of_stops
race_and_arrests
library(forcats)
race_and_arrests |>
ggplot( aes(x = fct_reorder(race, pct_arrest), y = pct_arrest)) +
geom_col(fill = "steelblue") +
labs(
title = "Arrest Rate by Race in Raleigh Traffic Stops",
x = "Race",
y = "Percent of Stops Ending in Arrest"
) +
theme_minimal() +
coord_flip()
sex_and_citations
race_and_arrests
library(forcats)
race_and_arrests |>
ggplot( aes(x = fct_reorder(race, pct_arrest), y = pct_arrest)) +
geom_col(fill = "steelblue") +
labs(
title = "Arrest Rate by Race in Raleigh Traffic Stops",
x = "Race",
y = "Percent of Stops Ending in Arrest"
) +
theme_minimal() +
coord_flip()
dbDisconnect(con_traffic, shutdown = TRUE)
reticulate::repl_python()
