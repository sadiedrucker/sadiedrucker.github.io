[
  {
    "objectID": "ethics_data_power.html",
    "href": "ethics_data_power.html",
    "title": "Ethics, Data, and Power",
    "section": "",
    "text": "OkCupid Data Breach: An Ethical Dilemma\n\nBackground:\nIn 2016, a group of Danish researchers collected data from the popular dating website OkCupid, which uses algorithms to match people based on their answers to personal questions (Resnick, 2016). The researchers scraped data from over 70,000 users’ profiles, collecting information such as usernames, age, gender, sexual orientation, political views, and responses to intimate survey questions (Resnick, 2016). They later published this data set online for use by other social scientists and data scientists. The data science component of this example was the creation of a large, open dataset to analyze human relationships through computational methods. The ethical dilemma emerged because this dataset contained identifiable and sensitive personal information collected without user consent or anonymization, exposing thousands of people to privacy violations (Xiao & Ma, 2021).\n\n\n\nWhere did the Danish researchers go wrong?\n\nConsent and Participant Awareness\nThe most significant ethical issue in this case concerns consent. None of the OkCupid users were informed that their data would be collected, analyzed, or shared publicly. Even though their profiles were technically visible online, users had created them for personal dating purposes, not to become research subjects. Informed consent requires that participants understand how their information will be used and have the opportunity to opt out. Here, informed consent was not possible because users were not contacted at all, and the researchers used data for purposes far beyond what users could have reasonably anticipated. This failure to seek consent violates both research ethics and user trust, as people were unknowingly exposed to potential harm through the release of their private information.\n\n\nPermission and Data Use Policies\nIn addition to lacking user consent, the researchers also did not obtain permission from OkCupid itself. The platform’s terms of service explicitly prohibit scraping or collecting user data without authorization. By ignoring these rules, the researchers bypassed the established permission structure for data use. In responsible research, permission structures help balance open scientific inquiry with the rights of data owners and subjects. In this case, the researchers justified their actions by saying that the data were “public,” but that reasoning overlooks the context of publication: users shared their profiles for dating, not for scientific analysis.\n\n\nAnonymity and Identifiability\nAnother major problem was that the released dataset was not anonymized. The researchers included usernames, locations, and detailed responses that could easily lead to re-identification. Since the data included highly sensitive information such as sexual orientation and political affiliation, a lack of anonymization posed real risks of exposure, discrimination, or harassment. Ethical data science requires either anonymizing data or adding “noise” (small distortions) to protect identities, but the researchers took no such precautions. Their failure to protect anonymity made the dataset deeply unethical to share.\n\n\nData Sharing and Accessibility\nThe researchers’ decision to make the dataset publicly available amplified the harm. They uploaded it to the Open Science Framework, where anyone could download it. Their intention may have been to promote transparency and open research, which are generally good data science values. However, in this case, openness came at the cost of privacy and safety. Sharing sensitive, identifiable data publicly transformed private individuals into involuntary research subjects. Instead of promoting scientific integrity, the decision undermined it by disregarding fundamental principles of respect and confidentiality.\n\n\n\nWhy does this matter?\nThis case matters because it exposes the tension between the ideals of open data and the rights of individuals to control their personal information as well as the issue of privacy on the internet. The people who benefited from this dataset were primarily researchers seeking to create this dataset. Those who were harmed were the OkCupid users, whose sensitive identities were exposed without their knowledge or consent. While there was no profit motive here, the ethical violations stemmed from a form of data power: the assumption that because data are accessible, they are available for use. This mindset reflects a broader problem in data science, prioritizing curiosity and discovery over responsibility and human dignity. The OkCupid case reminds us that just because data can be collected and analyzed does not mean it should be.\n\n\nSources\nResnick, B. (2016, May 12). Researchers just released profile data on 70,000 OkCupid users without permission. Vox. https://www.vox.com/2016/5/12/11666116/70000-okcupid-users-data-release\nXiao, T., & Ma, Y. (2021). A Letter to the Journal of Statistics and Data Science Education — A Call for Review of “OkCupid Data for Introductory Statistics and Data Science Courses” by Albert Y. Kim and Adriana Escobedo-Land. Journal of Statistics and Data Science Education, 29(2), 214–215. https://doi.org/10.1080/26939169.2021.1930812"
  },
  {
    "objectID": "NYTHeadlines.html",
    "href": "NYTHeadlines.html",
    "title": "New York Times Headlines",
    "section": "",
    "text": "Data source: New York Times Annotated Corpus, distributed with the RTextTools package\nTimothy P. Jurka, Loren Collingwood, Amber E. Boydstun, Emiliano Grossman and Wouter van Atteveldt (2012). RTextTools: Automatic Text Classification via Supervised Learning. R package version 1.3.9. http://CRAN.R-project.org/package=RTextTools.\nAccessed via built-in dataset NYTimes in R.\n\n\nCode\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(RTextTools) \nlibrary(forcats)\ndata(NYTimes)\nnyt&lt;-as_tibble(NYTimes)\n\nnyt&lt;-nyt |&gt;\n  mutate(Date=as.Date(Date, format = \"%d-%b-%y\"))\n\nnyt_clean &lt;- nyt |&gt;\n  mutate(clean_headline=Title |&gt;\n           str_to_lower()|&gt;\n           str_replace_all(\"[^a-z\\\\s]\", \" \")|&gt;\n           str_squish(),\n         year=year(Date)\n        )\n\nnyt_mentions &lt;- nyt_clean |&gt;\n  mutate(\n    clinton = str_detect(clean_headline, \"(?&lt;![a-z])clinton(?![a-z])\"),\n    bush = str_detect(clean_headline, \"(?&lt;![a-z])bush(?![a-z])\")\n  )\n\nnyt_periods &lt;- nyt_mentions |&gt;\n  mutate(\n    period = case_when(\n      Date &gt;= as.Date(\"1997-01-20\") & Date &lt; as.Date(\"2001-01-20\") ~ \"Clinton Era \n      (01/20/1997 to 01/20/2001)\",\n      Date &gt;= as.Date(\"2001-01-20\") & Date &lt; as.Date(\"2005-01-20\") ~ \"Bush era \n      (01/20/2001 to 01/20/2005)\",\n      TRUE ~ NA_character_\n    )\n  ) |&gt;\n  filter(!is.na(period))\n\nnyt_counts &lt;- nyt_periods |&gt;\n  group_by(period) |&gt;\n  summarise(\n    clinton_mentions = sum(clinton),\n    bush_mentions = sum(bush)\n  )\n\n  \n\nnyt_plot&lt;-nyt_counts|&gt;\n  pivot_longer(cols = c(clinton_mentions, bush_mentions),\n               names_to = \"name\", values_to = \"count\") \n \n\nggplot(nyt_plot, aes(x = period, y = count, fill = name)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"NYT Headline Mentions: Clinton vs Bush\",\n    x = \"Presidential Era\", \n    y = \"Number of Mentions\",\n    fill = \"President\"\n  ) +\n  scale_fill_manual(\n    values = c(\"clinton_mentions\" = \"blue\", \"bush_mentions\" = \"red\"),\n    labels = c(\"clinton_mentions\" = \"Clinton mentions\",\n               \"bush_mentions\" = \"Bush mentions\")\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis bar plot compares the total number of NYT headlines mentioning Clinton and Bush during their presidential terms. It shows that Bush received more media coverage overall than Clinton. This visualization highlights the relative attention each president received from the NYT and allows for a clear comparison between the two eras.\n\n\nCode\nnyt_trend &lt;- nyt_mentions |&gt;\n  filter(Date &gt;= as.Date(\"1997-01-20\") & Date &lt; as.Date(\"2005-01-20\")) |&gt;\n  mutate(month = floor_date(Date, \"month\")) |&gt;\n  group_by(month) |&gt;\n  summarise(\n    clinton = sum(clinton),\n    bush    = sum(bush)\n  ) |&gt;\n  pivot_longer(cols = c(clinton, bush),\n               names_to = \"name\",\n               values_to = \"count\")\n\nggplot(nyt_trend, aes(x = month, y = count, color = name)) +\n  geom_line(size = 1) +\n  geom_point(size = 2) +\n  scale_x_date(date_labels = \"%b %Y\", date_breaks = \"6 months\") + \n  scale_color_manual(\n    values = c(\"clinton\" = \"blue\", \"bush\" = \"red\"),\n    labels = c(\"clinton\" = \"Clinton mentions\",\n               \"bush\" = \"Bush mentions\")) +\n  \n  labs(\n    title = \"Monthly NYT Headline Mentions: Clinton vs Bush\",\n    x = \"Month\",\n    y = \"Number of Mentions\",\n    color = \"President\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1, size=8)) \n\n\n\n\n\n\n\n\n\nThis line plot shows how mentions of Clinton and Bush changed month by month throughout their presidencies. Spikes in the lines correspond to major events or news stories, such as elections, inaugurations, or controversies. It provides insight into the timing and dynamics of media attention, revealing trends that are not visible in the total counts alone."
  },
  {
    "objectID": "CDCdatasets.html",
    "href": "CDCdatasets.html",
    "title": "CDC Datasets",
    "section": "",
    "text": "Data curated by Jon Harmon for Tidy Tuesday from CDC datasets on Internet Archive.\n\n\nCode\ntuesdata &lt;- tidytuesdayR::tt_load('2025-02-11')\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(janitor)\nlibrary(httr2)\nlibrary(stringr)\n\nindex &lt;- rvest::read_html(\"https://archive.org/download/20250128-cdc-datasets\")\nmeta_urls &lt;- index |&gt; \n  rvest::html_element(\".download-directory-listing\") |&gt; \n  rvest::html_table() |&gt; \n  janitor::clean_names() |&gt; \n  dplyr::filter(stringr::str_ends(x1, \"-meta.csv\")) |&gt; \n  dplyr::mutate(\n    url = paste0(\n      \"https://archive.org/download/20250128-cdc-datasets/\",\n      URLencode(x1)\n    )\n  ) |&gt; \n  dplyr::select(url)\nrm(index)\n\n# As of 2025-02-03, there are 1257 metadata CSVs available. We will load each\n# one and widen it, then stitch them all together. This can take a very long\n# time.\nrequests &lt;- meta_urls$url |&gt;\n  purrr::map(\\(url) {\n    httr2::request(url) |&gt; \n      httr2::req_retry(\n        max_tries = 10,\n        is_transient = \\(resp) {\n          httr2::resp_status(resp) %in% c(429, 500, 503)\n        },\n        # Always wait 10 seconds to retry. It seems to be a general throttle,\n        # but they don't tell us how long they need us to back off.\n        backoff = \\(i) 10\n      )\n  })\n\nresps &lt;- httr2::req_perform_sequential(requests, on_error = \"continue\")\n\nreqs_to_retry &lt;- resps |&gt; \n  httr2::resps_failures() |&gt; \n  purrr::map(\"request\")\n\nresps2 &lt;- httr2::req_perform_sequential(reqs_to_retry)\n\nresps &lt;- c(httr2::resps_successes(resps), httr2::resps_successes(resps2))\n\nextract_cdc_dataset_row &lt;- function(resp) {\n  httr2::resp_body_string(resp) |&gt; \n    stringr::str_trim()\n}\n\ncdc_datasets &lt;- tibble::tibble(\n  dataset_url = purrr::map_chr(resps, c(\"request\", \"url\")),\n  raw = httr2::resps_data(resps, extract_cdc_dataset_row)\n) |&gt;\n  tidyr::separate_longer_delim(raw, delim = \"\\r\\n\") |&gt; \n  dplyr::filter(stringr::str_detect(raw, \",\")) |&gt; \n  tidyr::separate_wider_delim(\n    raw,\n    delim = \",\",\n    names = c(\"field\", \"value\"),\n    too_many = \"merge\",\n    too_few = \"align_start\"\n  ) |&gt; \n  # Remove opening/closing quotes and trailing commas.\n  dplyr::mutate(\n    value = stringr::str_trim(value),\n    value = dplyr::if_else(\n      stringr::str_starts(value, '\"') & stringr::str_ends(value, '\"') &\n        !stringr::str_detect(stringr::str_sub(value, 2, -2), '\"'),\n      stringr::str_sub(value, 2, -2),\n      value\n    ) |&gt; \n      stringr::str_remove(\",\\\\s*$\") |&gt; \n      dplyr::na_if(\"\") |&gt; \n      dplyr::na_if(\"NA\") |&gt;  \n      dplyr::na_if(\"n/a\") |&gt;  \n      dplyr::na_if(\"N/A\") \n  ) |&gt; \n  dplyr::distinct() |&gt; \n  dplyr::filter(!is.na(value)) |&gt; \n  tidyr::pivot_wider(\n    id_cols = c(dataset_url),\n    names_from = field,\n    values_from = value,\n    # Paste the contents of multi-value fields together.\n    values_fn = \\(x) {\n      paste(unique(x), collapse = \"\\n\")\n    }\n  ) |&gt; \n  janitor::clean_names() |&gt; \n  dplyr::mutate(\n    tags = purrr::map2_chr(tags, theme, \\(tags, theme) {\n      if (!is.na(theme)) {\n        paste(tags, theme, sep = \", \")\n      } else {\n        tags\n      }\n    }),\n    language = dplyr::case_match(\n      language,\n      \"English\" ~ \"en-US\",\n      .default = language\n    )\n  ) |&gt; \n  dplyr::mutate(\n    dplyr::across(\n      c(\"public_access_level\", \"update_frequency\"),\n      tolower\n    )\n  ) |&gt; \n  # Manually dropped identified meaningless columns.\n  dplyr::select(\n    -resource_name,\n    -system_of_records,\n    -theme,\n    -is_quality_data\n  )\n\nomb_codes &lt;- readr::read_csv(\"https://resources.data.gov/schemas/dcat-us/v1.1/omb_bureau_codes.csv\") |&gt; \n  janitor::clean_names() |&gt; \n  dplyr::mutate(\n    cgac_code = dplyr::na_if(cgac_code, \"n/a\")\n  )\n\nfpi_codes &lt;- readr::read_csv(\"https://resources.data.gov/schemas/dcat-us/v1.1/FederalProgramInventory_FY13_MachineReadable_091613.csv\") |&gt; \n  janitor::clean_names()\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\ndatasets_by_category &lt;- cdc_datasets |&gt;\n  group_by(category)|&gt;\n  summarize(n_datasets = n()) |&gt;\n  arrange(desc(n_datasets))\n\ntop_categories &lt;- head(datasets_by_category, 10)\n\nggplot(top_categories, aes(x = reorder(category, n_datasets), y = n_datasets)) +\n  geom_col(fill=\"pink\") +\n  coord_flip() +\n  labs(\n    title = \"Top 10 dataset categories\",\n    x = \"Category\",\n    y = \"Number of Datasets\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis plot displays the top ten categories for all datasets contained in the CDC datasets archive."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sadie Drucker",
    "section": "",
    "text": "Sadie Drucker is a third-year student at Scripps College.\n\n\nScripps College | Claremont, CA\nB.A in Biology and Data Science | Aug 2023 - Present"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Sadie Drucker",
    "section": "",
    "text": "Scripps College | Claremont, CA\nB.A in Biology and Data Science | Aug 2023 - Present"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Website created for DS 002R PO.01"
  },
  {
    "objectID": "WaterInsecurity.html",
    "href": "WaterInsecurity.html",
    "title": "Water Insecurity",
    "section": "",
    "text": "Data curated by Niha Pereira for Tidy Tuesday from U.S. Census Bureau data.\n\n\nCode\nlibrary(tidytuesdayR)\nlibrary(dplyr)\nlibrary(ggplot2)\ntuesdata &lt;- tidytuesdayR::tt_load('2025-01-28')\nwater_insecurity_2022 &lt;- tuesdata$water_insecurity_2022\nwater_insecurity_2023 &lt;- tuesdata$water_insecurity_2023\nwater_insecurity_2022 &lt;- water_insecurity_2022 |&gt; \n  dplyr::mutate(\n    geometry = purrr::map(geometry, \\(geo) {\n      eval(parse(text = geo))\n    } )\n  )\nwater_insecurity_2023 &lt;- water_insecurity_2023 |&gt; \n  dplyr::mutate(\n    geometry = purrr::map(geometry, \\(geo) {\n      eval(parse(text = geo))\n    } )\n  )\n\ntop10_lacking &lt;- water_insecurity_2022 |&gt;\n  mutate(greatest_lacking=percent_lacking_plumbing) |&gt;\n  arrange(desc(greatest_lacking)) |&gt;\n  slice_head(n=10)\n  \nggplot(top10_lacking, aes(x=reorder(name, greatest_lacking), y=greatest_lacking))+\n  geom_col(fill=\"pink\")+\n  coord_flip()+\n  labs(\n    title = \"Top 10 areas lacking plumbing in the U.S. in 2022\",\n    x=\"County Name\", \n    y=\"Percent Lacking Plumbing\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "Permutation.html",
    "href": "Permutation.html",
    "title": "Bill length in male vs female penguins",
    "section": "",
    "text": "This simulation study investigates whether there is a statistically significant difference in bill length between male and female penguins, testing the alternative hypothesis that bill length varies by sex. he analysis uses data from the palmerpenguins dataset in R, which contains morphological measurements for three penguin species from the Palmer Archipelago in Antarctica. Understanding differences in bill length across sexes can provide insights into sexual dimorphism and ecological adaptations in penguin species. A permutation test was conducted to simulate the distribution of mean differences under the null hypothesis that bill length is independent of sex, allowing for an assessment of whether the observed difference is more likely due to random chance."
  },
  {
    "objectID": "Permutation.html#background",
    "href": "Permutation.html#background",
    "title": "Bill length in male vs female penguins",
    "section": "",
    "text": "This simulation study investigates whether there is a statistically significant difference in bill length between male and female penguins, testing the alternative hypothesis that bill length varies by sex. he analysis uses data from the palmerpenguins dataset in R, which contains morphological measurements for three penguin species from the Palmer Archipelago in Antarctica. Understanding differences in bill length across sexes can provide insights into sexual dimorphism and ecological adaptations in penguin species. A permutation test was conducted to simulate the distribution of mean differences under the null hypothesis that bill length is independent of sex, allowing for an assessment of whether the observed difference is more likely due to random chance."
  },
  {
    "objectID": "Permutation.html#visualization-of-variables-of-interest",
    "href": "Permutation.html#visualization-of-variables-of-interest",
    "title": "Bill length in male vs female penguins",
    "section": "Visualization of variables of interest",
    "text": "Visualization of variables of interest\n\n\nCode\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(ggplot2)\n\nclean_penguins &lt;- penguins |&gt;\n  filter(!is.na(bill_length_mm), !is.na(sex))\n\nggplot(clean_penguins, aes(x=sex, y=bill_length_mm, fill=sex))+\n  geom_boxplot()+\n    labs(\n      title = \"Bill length in male vs female penguins\" , \n      x= \"Sex\",\n      y= \"Bill length in mm\"\n      )+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nIn the original data presented in the above graph, male penguins have a greater median bill length. To reject the null hypothesis, we performed a permutation test to determine whether this observed difference between sexes is likely to have occurred by chance."
  },
  {
    "objectID": "Permutation.html#permutation-test",
    "href": "Permutation.html#permutation-test",
    "title": "Bill length in male vs female penguins",
    "section": "Permutation Test",
    "text": "Permutation Test\n\nStep 1: Define the null hypothesis\nIn this case, the null hypothesis is that bill length is independent of sex in penguins.\n\n\nStep 3: Permute (shuffle) the variable labels\nThis forces the null hypothesis to be true.\n\n\nStep 3: Compute the observed test statistic on the null sampling distribution.\nWe do this by computing the mean and median differences between the sexes. I created a function called perm_data(). This function keeps track of the current replication number, rep, and the data set containing sex and bill_length_mm.The function mutates the data set to add a column called sex_perm. that randomly shuffles the sex labels. This action simulates the null hypothesis.\nThe mapping function quickly tests if the function is working as expected."
  },
  {
    "objectID": "Permutation.html#section",
    "href": "Permutation.html#section",
    "title": "Bill length in male vs female penguins",
    "section": "",
    "text": "Code\n#Permutation test \n\nset.seed(47) \n\nperm_data &lt;- function(rep, data){\n  data |&gt; \n    select(sex, bill_length_mm) |&gt; \n    mutate(sex_perm = sample(sex, replace = FALSE)) |&gt;\n    summarize(\n      obs_ave_diff = mean(bill_length_mm[sex == \"male\"], na.rm = TRUE) - \n                     mean(bill_length_mm[sex == \"female\"], na.rm = TRUE),\n      obs_median_diff = median(bill_length_mm[sex == \"male\"], na.rm = TRUE) - \n                        median(bill_length_mm[sex == \"female\"], na.rm = TRUE),\n      perm_ave_diff = mean(bill_length_mm[sex_perm == \"male\"], na.rm = TRUE) - \n                      mean(bill_length_mm[sex_perm == \"female\"], na.rm = TRUE),\n      perm_median_diff = median(bill_length_mm[sex_perm == \"male\"], na.rm = TRUE) - \n                         median(bill_length_mm[sex_perm == \"female\"], na.rm = TRUE),\n              rep = rep)\n}\n  \nmap(c(1:10), perm_data, data = clean_penguins) |&gt; \n  list_rbind()\n\n\n# A tibble: 10 × 5\n   obs_ave_diff obs_median_diff perm_ave_diff perm_median_diff   rep\n          &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt; &lt;int&gt;\n 1         3.76               4        -0.245            1.60      1\n 2         3.76               4        -1.33            -2.55      2\n 3         3.76               4        -0.396           -0.950     3\n 4         3.76               4         0.634            1.80      4\n 5         3.76               4         0.288            1.75      5\n 6         3.76               4        -0.102           -0.850     6\n 7         3.76               4         0.340            1.05      7\n 8         3.76               4        -0.933           -2.6       8\n 9         3.76               4        -0.383           -1.85      9\n10         3.76               4        -0.643           -1.30     10\n\n\n\nStep 4: Visualize null sampling distributions\nThe mapping function used here runs the function perm_data() one thousand times to simulate the permutation distribution.\nEach row in perm_stats represents one shuffled permutation: the permuted mean and median differences and the observed statistics.\n\n\nCode\nset.seed(47)\nperm_stats &lt;- \n  map(c(1:1000), perm_data, data = clean_penguins) |&gt; \n  list_rbind() \n\nperm_stats |&gt; \n  ggplot(aes(x = perm_ave_diff)) + \n  geom_histogram() + \n  geom_vline(aes(xintercept = obs_ave_diff), color = \"red\")+\n  labs(\n    title=\"Mean null sampling distributions \"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nperm_stats |&gt; \n  ggplot(aes(x = perm_median_diff)) + \n  geom_histogram() + \n  geom_vline(aes(xintercept = obs_median_diff), color = \"red\")+\n   labs(\n    title=\"Median null sampling distributions \"\n  )\n\n\n\n\n\n\n\n\n\n\n\nStep 5: Compute the p-value\nThis value indicates the probability of observing the data or more extreme outcomes if the null hypothesis is true. In other words, the probability that the pattern in the original observed data is due to random chance.\n\n\nCode\n perm_stats |&gt; \n  summarize(\n    p_val_ave = (sum(abs(perm_ave_diff) &gt;= abs(obs_ave_diff[1])) + 1) / (n() + 1),\n    p_val_median = (sum(abs(perm_median_diff) &gt;= abs(obs_median_diff[1])) + 1) / (n() + 1)\n  ) \n\n\n# A tibble: 1 × 2\n  p_val_ave p_val_median\n      &lt;dbl&gt;        &lt;dbl&gt;\n1  0.000999     0.000999"
  },
  {
    "objectID": "Permutation.html#conclusion",
    "href": "Permutation.html#conclusion",
    "title": "Bill length in male vs female penguins",
    "section": "Conclusion",
    "text": "Conclusion\nThe permutation test provides strong evidence that male and female penguins have different bill lengths. Both the mean and median differences are far outside the distribution expected under the null hypothesis of no difference. The extremely small p-values (p ≈ 0.001) indicate that the observed differences are highly unlikely to have occurred by random chance, confirming significant sexual dimorphism in bill length."
  }
]