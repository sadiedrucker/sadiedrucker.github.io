[
  {
    "objectID": "WaterInsecurity.html",
    "href": "WaterInsecurity.html",
    "title": "Water Insecurity",
    "section": "",
    "text": "Data curated by Niha Pereira for Tidy Tuesday from U.S. Census Bureau data.\n\n\nCode\nlibrary(tidytuesdayR)\nlibrary(dplyr)\nlibrary(ggplot2)\ntuesdata &lt;- tidytuesdayR::tt_load('2025-01-28')\nwater_insecurity_2022 &lt;- tuesdata$water_insecurity_2022\nwater_insecurity_2023 &lt;- tuesdata$water_insecurity_2023\nwater_insecurity_2022 &lt;- water_insecurity_2022 |&gt; \n  dplyr::mutate(\n    geometry = purrr::map(geometry, \\(geo) {\n      eval(parse(text = geo))\n    } )\n  )\nwater_insecurity_2023 &lt;- water_insecurity_2023 |&gt; \n  dplyr::mutate(\n    geometry = purrr::map(geometry, \\(geo) {\n      eval(parse(text = geo))\n    } )\n  )\n\ntop10_lacking &lt;- water_insecurity_2022 |&gt;\n  mutate(greatest_lacking=percent_lacking_plumbing) |&gt;\n  arrange(desc(greatest_lacking)) |&gt;\n  slice_head(n=10)\n  \nggplot(top10_lacking, aes(x=reorder(name, greatest_lacking), y=greatest_lacking))+\n  geom_col(fill=\"pink\")+\n  coord_flip()+\n  labs(\n    title = \"Top 10 areas lacking plumbing in the U.S. in 2022\",\n    x=\"County Name\", \n    y=\"Percent Lacking Plumbing\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Website created for DS 002R PO.01"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sadie Drucker",
    "section": "",
    "text": "Sadie Drucker is a third-year student at Scripps College.\n\n\nScripps College | Claremont, CA\nB.A in Biology and Data Science | Aug 2023 - Present"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Sadie Drucker",
    "section": "",
    "text": "Scripps College | Claremont, CA\nB.A in Biology and Data Science | Aug 2023 - Present"
  },
  {
    "objectID": "CDCdatasets.html",
    "href": "CDCdatasets.html",
    "title": "CDC Datasets",
    "section": "",
    "text": "Data curated by Jon Harmon for Tidy Tuesday from CDC datasets on Internet Archive.\n\n\nCode\ntuesdata &lt;- tidytuesdayR::tt_load('2025-02-11')\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(janitor)\nlibrary(httr2)\nlibrary(stringr)\n\nindex &lt;- rvest::read_html(\"https://archive.org/download/20250128-cdc-datasets\")\nmeta_urls &lt;- index |&gt; \n  rvest::html_element(\".download-directory-listing\") |&gt; \n  rvest::html_table() |&gt; \n  janitor::clean_names() |&gt; \n  dplyr::filter(stringr::str_ends(x1, \"-meta.csv\")) |&gt; \n  dplyr::mutate(\n    url = paste0(\n      \"https://archive.org/download/20250128-cdc-datasets/\",\n      URLencode(x1)\n    )\n  ) |&gt; \n  dplyr::select(url)\nrm(index)\n\n# As of 2025-02-03, there are 1257 metadata CSVs available. We will load each\n# one and widen it, then stitch them all together. This can take a very long\n# time.\nrequests &lt;- meta_urls$url |&gt;\n  purrr::map(\\(url) {\n    httr2::request(url) |&gt; \n      httr2::req_retry(\n        max_tries = 10,\n        is_transient = \\(resp) {\n          httr2::resp_status(resp) %in% c(429, 500, 503)\n        },\n        # Always wait 10 seconds to retry. It seems to be a general throttle,\n        # but they don't tell us how long they need us to back off.\n        backoff = \\(i) 10\n      )\n  })\n\nresps &lt;- httr2::req_perform_sequential(requests, on_error = \"continue\")\n\nreqs_to_retry &lt;- resps |&gt; \n  httr2::resps_failures() |&gt; \n  purrr::map(\"request\")\n\nresps2 &lt;- httr2::req_perform_sequential(reqs_to_retry)\n\nresps &lt;- c(httr2::resps_successes(resps), httr2::resps_successes(resps2))\n\nextract_cdc_dataset_row &lt;- function(resp) {\n  httr2::resp_body_string(resp) |&gt; \n    stringr::str_trim()\n}\n\ncdc_datasets &lt;- tibble::tibble(\n  dataset_url = purrr::map_chr(resps, c(\"request\", \"url\")),\n  raw = httr2::resps_data(resps, extract_cdc_dataset_row)\n) |&gt;\n  tidyr::separate_longer_delim(raw, delim = \"\\r\\n\") |&gt; \n  dplyr::filter(stringr::str_detect(raw, \",\")) |&gt; \n  tidyr::separate_wider_delim(\n    raw,\n    delim = \",\",\n    names = c(\"field\", \"value\"),\n    too_many = \"merge\",\n    too_few = \"align_start\"\n  ) |&gt; \n  # Remove opening/closing quotes and trailing commas.\n  dplyr::mutate(\n    value = stringr::str_trim(value),\n    value = dplyr::if_else(\n      stringr::str_starts(value, '\"') & stringr::str_ends(value, '\"') &\n        !stringr::str_detect(stringr::str_sub(value, 2, -2), '\"'),\n      stringr::str_sub(value, 2, -2),\n      value\n    ) |&gt; \n      stringr::str_remove(\",\\\\s*$\") |&gt; \n      dplyr::na_if(\"\") |&gt; \n      dplyr::na_if(\"NA\") |&gt;  \n      dplyr::na_if(\"n/a\") |&gt;  \n      dplyr::na_if(\"N/A\") \n  ) |&gt; \n  dplyr::distinct() |&gt; \n  dplyr::filter(!is.na(value)) |&gt; \n  tidyr::pivot_wider(\n    id_cols = c(dataset_url),\n    names_from = field,\n    values_from = value,\n    # Paste the contents of multi-value fields together.\n    values_fn = \\(x) {\n      paste(unique(x), collapse = \"\\n\")\n    }\n  ) |&gt; \n  janitor::clean_names() |&gt; \n  dplyr::mutate(\n    tags = purrr::map2_chr(tags, theme, \\(tags, theme) {\n      if (!is.na(theme)) {\n        paste(tags, theme, sep = \", \")\n      } else {\n        tags\n      }\n    }),\n    language = dplyr::case_match(\n      language,\n      \"English\" ~ \"en-US\",\n      .default = language\n    )\n  ) |&gt; \n  dplyr::mutate(\n    dplyr::across(\n      c(\"public_access_level\", \"update_frequency\"),\n      tolower\n    )\n  ) |&gt; \n  # Manually dropped identified meaningless columns.\n  dplyr::select(\n    -resource_name,\n    -system_of_records,\n    -theme,\n    -is_quality_data\n  )\n\nomb_codes &lt;- readr::read_csv(\"https://resources.data.gov/schemas/dcat-us/v1.1/omb_bureau_codes.csv\") |&gt; \n  janitor::clean_names() |&gt; \n  dplyr::mutate(\n    cgac_code = dplyr::na_if(cgac_code, \"n/a\")\n  )\n\nfpi_codes &lt;- readr::read_csv(\"https://resources.data.gov/schemas/dcat-us/v1.1/FederalProgramInventory_FY13_MachineReadable_091613.csv\") |&gt; \n  janitor::clean_names()\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\ndatasets_by_category &lt;- cdc_datasets |&gt;\n  group_by(category)|&gt;\n  summarize(n_datasets = n()) |&gt;\n  arrange(desc(n_datasets))\n\ntop_categories &lt;- head(datasets_by_category, 10)\n\nggplot(top_categories, aes(x = reorder(category, n_datasets), y = n_datasets)) +\n  geom_col(fill=\"pink\") +\n  coord_flip() +\n  labs(\n    title = \"Top 10 dataset categories\",\n    x = \"Category\",\n    y = \"Number of Datasets\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis plot displays the top ten categories for all datasets contained in the CDC datasets archive."
  }
]